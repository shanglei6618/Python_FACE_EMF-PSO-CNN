{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ce3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K  \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "%matplotlib inline\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv('./filename.csv')\n",
    "\n",
    "\n",
    "labels = pd.factorize(data.pop('species'))\n",
    "y = labels[0]\n",
    "_ = data.pop('id')\n",
    "x = data\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y)\n",
    "# train_x.shape, test_x.shape\n",
    "\n",
    "mean = train_x.mean(axis=0)\n",
    "std = train_x.std(axis=0)\n",
    "\n",
    "# Data standardization\n",
    "train_x_norm = (train_x - mean)/std\n",
    "test_x_norm = (test_x - mean)/std\n",
    "\n",
    "train_x_norm = np.expand_dims(train_x_norm, -1)\n",
    "# train_x_norm.shape\n",
    "\n",
    "test_x_norm = np.expand_dims(test_x_norm, -1)\n",
    "# test_x_norm.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24101e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition model\n",
    "def model_design(x):\n",
    "    print(f\"The number of neurons is : {int(x[0])}\")\n",
    "    K.clear_session()  # Clear a previously existing calculation diagram\n",
    "    # Define the number of model layers\n",
    "    inputs = layers.Input(shape=train_x_norm.shape[1:])\n",
    "    z = layers.Conv1D(64, 11, activation='relu', padding='same')(inputs)\n",
    "    z = layers.Conv1D(64, 11, activation='relu', padding='same')(z)\n",
    "    z = layers.MaxPooling1D(3)(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    z = layers.Conv1D(128, 11, activation='relu', padding='same')(z)\n",
    "    z = layers.Conv1D(128, 11, activation='relu', padding='same')(z)\n",
    "    z = layers.MaxPooling1D(1)(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    z = layers.Conv1D(256, 11, activation='relu', padding='same')(z)\n",
    "    z = layers.Conv1D(256, 11, activation='relu', padding='same')(z)\n",
    "    z = layers.MaxPooling1D(1)(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    z = layers.Conv1D(512, 11, activation='relu', padding='same')(z)\n",
    "    z = layers.Conv1D(512, 11, activation='relu', padding='same')(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    z = layers.GlobalAveragePooling1D()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    z = layers.Dense(256, activation='relu')(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    z = layers.Dense(int(x[0]), activation='relu')(z)\n",
    "    z = layers.Dense(3, activation='softmax')(z)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=z)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc']\n",
    "             )\n",
    "    model.summary()\n",
    "    return model  # return model\n",
    "\n",
    "                      \n",
    "# Define a model training function\n",
    "def best_model(x):\n",
    "    model = model_design(x)  # Invoke the model design function\n",
    "    history = model.fit(train_x_norm, train_y, epochs =300 ,\n",
    "                    validation_data = (test_x_norm, test_y),\n",
    "#                     callbacks = [tf_callback,callback],\n",
    "                    batch_size = 256, verbose = 1)  \n",
    "\n",
    "    val_loss = model.evaluate(test_x_norm, test_y, verbose=0)\n",
    "    print(f\"train loss: {val_loss}\")\n",
    "\n",
    "    return val_loss[0]\n",
    "\n",
    "\n",
    "# Import pso particle swarm module\n",
    "from pyswarm import pso  \n",
    "# Defined upper and lower limits\n",
    "lowerb = [32]\n",
    "upperb = [64]\n",
    "\n",
    "# The pso particle swarm optimization algorithm is invoked \n",
    "xopt, fopt = pso(func=best_model, lb=lowerb, ub=upperb, maxiter=1, swarmsize=2)\n",
    "print('Optimal number of neuronsï¼š', int(xopt[0]))\n",
    "\n",
    "# After executing this code, pso will optimize the number of neurons in the fully connected layer, and after several rounds of optimization, the optimal number of neurons is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe423bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke model\n",
    "model = model_design(x = [38])  # Input the optimized number of neurons into the model\n",
    "history = model.fit(train_x_norm, train_y, epochs =600 ,\n",
    "                validation_data = (test_x_norm, test_y),\n",
    "#                     callbacks = [tf_callback,callback],\n",
    "                batch_size = 256, verbose = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw\n",
    "plt.plot(history.epoch, history.history.get('acc'), 'y', label='Training acc')\n",
    "plt.plot(history.epoch, history.history.get('val_acc'), 'b', label='Test acc')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
